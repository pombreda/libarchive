#summary Ideas that would be nice to implement someday.

<wiki:toc>

= Command-line parsing for compress_program =

The `archive_read_support_compress_program()` and `archive_write_set_compress_program()`
use external programs to compress or decompress data.
Right now, those take their string argument as the name of a program to run,
which provides no way to run programs with arguments.
This is why libarchive currently uses "gunzip", for example, as the fallback
for decompressing gzip streams instead of the more portable "gzip -d".
The parsing requirements are not difficult, so this should be a relatively
easy project for someone.

= ZIsoFS support =

See Issue 2 for details.

= Extended Attribute interoperability =

libarchive's handling of POSIX.1e-style extended attributes
in pax files should be interoperable across systems,
but there has not been a lot of testing of this.
There also needs to be some work to implement the system-dependent
portions for more operating systems.  (Currently, only Linux and
FreeBSD are fully supported; it should be easy to add Mac OS and
Solaris and possibly others.)

= Pax front-end =

It should be feasible to build a POSIX-compliant pax on top of libarchive.

= Sparse archiving =

FreeBSD now has `seek(HOLE)` support that should make it possible
to efficiently archive sparse files.
This will require adding a new archive_entry field to store the
list of segments and modifications to archive_read_disk and
the pax writer.

= cpio as archive translation =

Essentially all of cpio's operation can be viewed as
archive translation (where "archive" means "description
of a series of filesystem objects"):
 * *cpio -o* reads a list of filenames and writes a cpio or tar archive
 * *cpio -p* reads a list of filenames and writes filesystem objects to disk
 * *cpio -i* reads an archive and writes filesystem objects to disk
libarchive's mtree reader demonstrates that a list of filenames
can be viewed as an archive, and `archive_write_disk()`
shows that a series of filesystem objects can be written to
a disk directory using the same interface that is used to
write them to a tar or cpio archive.
By implementing a "list of filenames" reader (similar to
the existing mtree reader), it should be possible to dramatically
simplify the cpio implementation.
Some tweaks to libarchive APIs might be necessary to make
this really clean, of course.
The end result should be a cpio that does everything the
standard requires with a few nice new features:
 * *cpio -o* can do arbitrary archive translation; if it can read a list of filenames, there's no reason it can't read a tar archive as well.
 * `find | cpio` can use mtree tricks.  In particular, using mtree format between find and cpio allows the use of sed, grep, and awk to modify metadata (e.g., create a ustar archive where every file is owned by root).
 * A more modular libarchive interface will be more useful to other applications; this exercise will help in identifying some of the remaining rough edges.  (In particular, implementing *cpio -pl* efficiently will require some careful thought about how `sourcepath` should be handled.)

= Virtualizing archive_entry =

The username and uid lookups are awkward.
These lookups are expensive to do and difficult to avoid.
The assumption that they can be pushed into `archive_write_disk()` and `archive_read_disk()` works well enough for tar and cpio because they
operate in a particular fashion.
Other uses won't necessarily operate in the same way.
One way to solve this is to allow archive readers to push lookup
functions into the entry instead of direct values.
This would allow the username/group name/gid/uid lookup
to occur only when the value was requested.
This will likely require some rethinking of libarchive APIs.
In particular, the capability to set lookup functions may need
to be pushed into the generic "struct archive" so that it can
be available to every reader or writer.

Similar concerns apply to ACLs and extended attributes;
it would be most efficient to entirely skip such lookups
if the entry is ultimately being written to an archive format
that can't utilize such information.

= Directory traversals in archive_read_disk =

Libarchive is a toolkit for handling long lists of filesystem
objects.
The most obvious glaring hole is that you cannot ask libarchive
to enumerate the filesystem objects in a directory tree.
The `archive_read_disk` API is the obvious place to put this
capability.
Getting an initial implementation working should be straightforward;
just copy the "tree.c" code from bsdtar and extend it to
return a series of archive_entry objects instead of just filenames.
The archive_entry virtualization above would help here but
is not a prerequisite.

Longer term, there's a hard problem here around file filtering.
Tools such as tar want to filter the files they look at; doing
this efficiently requires doing it as early as possible.
One way to implement this would be to add a set of
"archive_filter" objects that could be attached to an
archive_read_disk object or any other archive reader.
Then the bsdtar program could simply instantiate these
filters, attach them to an archive_read_disk, then
feed entries from archive_read_disk to a suitably
configured output archive.
This would dramatically simplify bsdtar and should
also make it easier for people to use libarchive
in complex ways elsewhere (e.g., people building
backup tools will want to avoid backup files
or files marked "nodump" and would benefit from
having easy ways to just turn on such capabilities).

= Explicit read pipelines =

Today, read filters are configured and then read pipelines
are assembled automatically.
There are cases where clients would like to create the
read pipeline explicitly.
This would allow them to handle partially-damaged input,
for example, by skipping the bidding process.
This probably requires rephrasing the existing read
filters so that there is an `archive_read_add_filter_XXXX()`
as well as an `archive_read_support_filter_XXXX()`.
In this form, the _support_ form would register the
bidder and the _add_ function.
The bid process would automatically invoke the _add_
function for the winning bidder.
Clients who wanted explicit configuration would just
call the _add_ version directly to add that filter
directly to the pipeline.

= Stackable write filters =

The write filters should be restructured so that you
can have multiple write filters.
We can then separate out the blocking into a separate
filter and eliminate blocking issues from all of the
compression filters.
This would also enable some new write capabilities that
I'm very interested in.

= UUencode/decode filters =

On the write side, this requires stackable write filters first.
On the read side, this should be quite routine.

= Encrypted backup support =

This requires stackable write filters first.
I have a rough design of how to implement this; ask for details.

= Exploiting seekable input =

This requires a little care, but ISO and Zip readers could really
benefit from seeking when the input supports it.
I insist that we maintain the ability to read most ISO and Zip
files even when seek is unavailable.

= Seek in archives =

A few people have asked for the ability to efficiently
"re-read" particular archive entries.
This is a tricky subject.
For many formats, the performance gains from this would
be very modest.
For example, after the item above is implemented, re-reading
a Zip archive from the beginning will be very fast since
it would only involve re-parsing the central directory.
The cases where there would be real gains (e.g., tar.gz)
are going to be very difficult to handle.
The most likely implementation would be some form of checkpointing
so that clients can explicitly ask for a checkpoint object and then
restore back to that checkpoint.
The checkpoint object could be complex if you have a series of
stacked read filters plus state in the format handler itself.

= MMap and async I/O performance experiments =

It should be possible to read archives using mmap()
or async I/O and it might be significantly faster than
the current approach.
However, this requires some careful performance testing
before we can be confident that it really is an improvement.
(If it doesn't help performance, we shouldn't do it.)

Similar improvements when writing archives are feasible,
though there are a few issues in the current write blocking that
make this harder than it sounds.
(The stackable write filter project above would
help; that would at least consolidate blocking into
fewer places.)

= ISO writer =

ISO writing is tricky because all metadata precedes all file data.
I've figured out a couple of easy workarounds, though, so I think this
is manageable.
The hardest part will be implementing this in a way that avoids the
need to have a lot of options.  The `mkisofs` utility already does
a good job of providing lots of options for people who need specific
control over how their ISO images are created; I'd like for libarchive
to provide a solid, easy-to-use answer for the 99% of the time
that people just need an ISO image that works.
(I envision that libarchive's ISO writer would generate Joliet
and Rockridge extensions and would provide a very few
options for creating bootable CDs.)

= Mac "!CopyFile()" support =

It would be nice for bsdtar to be compatible with Apple's
modified GNU tar implementation.

= xar reader and writer =

Xar should be relatively straightforward to read.
Writing will require tricks similar to the ISO writer.

= BSD-style long filename support for GNU ld =

The GNU/SysV ar format is ugly to write because you need to collect a filename table in advance.
This complicates programs that write ar format.
The BSD ar format avoids this problem but GNU ld doesn't support it.
If GNU ld could read the BSD ar format, then it would be easier
to create library-management tools on top of libarchive.