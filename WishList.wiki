#summary Ideas that would be nice to implement someday.

If you would like to see any of these implemented, let us know.
If you'd like to take a stab at implementing any of these, definitely let us know.
Also see [ReleaseNotes].
Please remember that libarchive is entirely developed by volunteers;
new features get implemented whenever someone steps up to do the work.

<wiki:toc>

= Better Build Documentation =

The existing build documentation is a little hard to navigate.
It would be nice to break it out into separate pages by platform and provide more in the way of screenshots for the graphical tools and command examples for the command-line tools.

= Command-line parsing for compress_program =

The `archive_read_support_compress_program()` and `archive_write_set_compress_program()`
use external programs to compress or decompress data.
Right now, those take their string argument as the name of a program to run,
which provides no way to run programs with arguments.
This is why libarchive currently uses "gunzip", for example, as the fallback
for decompressing gzip streams instead of the more portable "gzip -d".
The parsing requirements are not difficult, so this should be a relatively
easy project for someone.

= ZIsoFS support =

See Issue 2 for details.

= Extended Attribute interoperability =

libarchive's handling of POSIX.1e-style extended attributes
in pax files should be interoperable across systems,
but there has not been a lot of testing of this.
There also needs to be some work to implement the system-dependent
portions for more operating systems.  (Currently, only Linux and
FreeBSD are fully supported; it should be easy to add Mac OS and
Solaris and possibly others.)

= Pax front-end =

It should be feasible to build a POSIX-compliant pax on top of libarchive.

= Sparse archiving =

FreeBSD now has `seek(HOLE)` support that should make it possible
to efficiently archive sparse files.
This will require adding a new archive_entry field to store the
list of segments and modifications to archive_read_disk and
the pax writer.

= cpio as archive translation =

Essentially all of cpio's operation can be viewed as
archive translation (where "archive" means "description
of a series of filesystem objects"):
 * *cpio -o* reads a list of filenames and writes a cpio or tar archive
 * *cpio -p* reads a list of filenames and writes filesystem objects to disk
 * *cpio -i* reads an archive and writes filesystem objects to disk
libarchive's mtree reader demonstrates that a list of filenames
can be viewed as an archive, and `archive_write_disk()`
shows that a series of filesystem objects can be written to
a disk directory using the same interface that is used to
write them to a tar or cpio archive.
By implementing a "list of filenames" reader (similar to
the existing mtree reader), it should be possible to dramatically
simplify the cpio implementation.
Some tweaks to libarchive APIs might be necessary to make
this really clean, of course.
The end result should be a very simple cpio implementation that does everything the
standard requires with a few nice new features:
 * *cpio -o* can do arbitrary archive translation; if it can read a list of filenames, there's no reason it can't read a tar archive as well.
 * The common `find|cpio` idiom can use mtree format.  In particular, using mtree format between find and cpio allows the use of sed, grep, and awk to modify metadata (e.g., create a ustar archive where every file is owned by root).
 * A more modular libarchive interface will be more useful to other applications; this exercise will help in identifying some of the remaining rough edges.  (In particular, implementing *cpio -pl* efficiently will require some careful thought about how `sourcepath` should be handled.)

= Virtualizing archive_entry =

The username, group name, uid, and gid lookups are awkward.
These lookups are expensive to do so we'd like to avoid them when we can.
Pushing them into `archive_write_disk()` and `archive_read_disk()` works
well enough for tar and cpio because they operate in a particular fashion.
Other uses won't necessarily operate in the same way.

One way to solve this is to allow archive readers to push lookup
functions into the entry instead of direct values.
This would allow the username/group name/gid/uid lookup
to occur only when the value was requested.
If it was never requested, the lookup would never happen.
This will likely require some rethinking of libarchive APIs.
In particular, the existing "set_lookup" capabilities may need
to be reworked as a generic facility that can be configured
for any archive object, not just read_disk/write_disk objects.

Similar concerns apply to ACLs and extended attributes;
it would be most efficient to entirely skip such lookups
if the entry is ultimately being written to an archive format
that can't utilize such information.
(For example, using archive_read_disk() to get entries to
feed to a cpio writer currently pulls ACLs, extended attributes,
and user/group names that are never actually used.)

= Directory traversals in archive_read_disk =

Libarchive is a toolkit for handling long lists of filesystem objects.
The most obvious glaring hole is that you cannot ask libarchive
to enumerate the filesystem objects in a directory tree.
The `archive_read_disk` API is the obvious place to put this capability.
Getting an initial implementation working should be straightforward;
just copy the "tree.c" code from bsdtar and extend it to
return a series of archive_entry objects instead of just filenames.
The archive_entry virtualization above would help here but
is not a prerequisite.

Longer term, there's a hard problem here around file filtering.
Tools such as tar want to filter the files they see; doing
this efficiently requires doing it as early as possible.
(In particular, filesystem traversals would like to avoid
descending into directory trees if nothing in that tree
will ever be visited.)
One way to implement this would be to add a set of
"archive_filter" objects that could be attached to an
archive_read_disk object or any other archive reader.
Then the bsdtar program could simply instantiate these
filters, attach them to an archive_read_disk, then
feed entries from archive_read_disk to a suitably
configured output archive.
This would dramatically simplify bsdtar and should
also make it easier for people to use libarchive
in complex ways elsewhere (e.g., people building
backup tools will likely want to avoid backup files
or files marked "nodump" and would benefit from
having easy ways to just turn on such capabilities).

= Explicit read pipelines =

Today, read filters are configured and then read pipelines
are assembled automatically.
There are a few cases where clients would like to create the
read pipeline manually.
This would allow them to handle partially-damaged input,
for example, by skipping the bidding process.
This probably requires rephrasing the existing read
filters so that there is an `archive_read_add_filter_XXXX()`
as well as an `archive_read_support_filter_XXXX()`.
In this form, the _support_ form would register the
bidder and the _add_ function.
The bid process would automatically invoke the _add_
function for the winning bidder.
Clients who wanted explicit configuration would just
call the _add_ version directly to add that filter
directly to the pipeline.

Of course, all of the capability here already exists;
the work here is to cleanly expose things, develop tests,
and make sure that the final API is clean and easy to
understand (and backwards compatible with existing clients
if that's possible).

= Stackable write filters =

The write filters should be restructured so that you
can have multiple write filters.
We can then separate out the blocking into a separate
filter and eliminate blocking issues from all of the
compression filters.
This would also enable some new write capabilities that
I'm very interested in, including uuencode/uudecode support,
mime/base-64 support, and encrypted backup handling.

= UUencode/decode filters =

On the write side, this requires stackable write filters first.
On the read side, the framework already exists; it just
needs a new filter to be written and added.

= Encrypted backup support =

This requires stackable write filters first but
should otherwise be a straightforward use of OpenSSL.
I do want to have a standalone pair of encrypt/decrypt
command-line tools in addition to the read/write filters
built into libarchive.
I have a rough design of how to implement this; ask for details.

= Multivolume Writing =

This doesn't explicitly require stackable write filters,
but implementing it first would complicate the stackable
write filter implementation, so I'd rather see the stackable
writer filters finished first.
This requires two changes to the internal write filter API:
 * A "bytes remaining" call that the format writer can make against the write filters.  Most write filters will just pass it downstream and possibly reduce the return value to account for data they have currently buffered (and any end-of-file overhead).  Compression filters shouldn't try to adjust for compression ratio; underestimates are okay here but overestimates can be fatal.
 * A "volume change" call that the format writer can make to prompt each write filter to flush, invoke it's downstream, then reinitialize.

The interesting part will be working through the format writers to actually support this.
For tar and cpio, it might be as simple as invoking
a volume change before any entry that is larger than
the remaining bytes (plus some overhead).
(GNU tar-like entry splitting is not necessary in the first version.)
It is likely possible to implement multi-volume support for other
formats as well.
Here is an interesting use case:  Two libarchive instances, the
first writing a compressed tar stream, the second creating ISO images,
with volumes split so that each ISO image has a single compressed
tar file that fills the ISO.
The volume change from the tar writer closes the ISO writer, invokes
an ISO burner program to burn a disk, then reopens the ISO writer.
(Bonus points, of course, if the burner program runs while the next ISO is being
created.)

= RMT support =

RMT hasn't been a priority simply because noone has ever asked for it.
However, it is one of the few features that is both
reasonably standard across other tar implementations and
already has a hook for adding it to libarchive (just
add a new `archive_read_open_rmt()` module).
NetBSD has a BSD-licensed rmt library that we might be able to
crib from, though the rmt protocol is simple
enough that you could also just implement it from scratch.
This will require careful compatibility testing with a
couple of different rmt servers.

This would be a good project for someone who is trying
to learn network programming.

= Exploiting seekable input =

This requires a little care, but ISO and Zip readers could really
benefit from seeking when the input supports it.
I insist that we maintain the ability to read most ISO and Zip
files even when seek is unavailable.

= Seek in archives =

A few people have asked for the ability to efficiently
"re-read" particular archive entries.
This is a tricky subject.
For many formats, the performance gains from this would
be very modest.
For example, after the item above is implemented, re-reading
a Zip archive from the beginning will be very fast since
it would only involve re-parsing the central directory.
The cases where there would be real gains (e.g., tar.gz)
are going to be very difficult to handle.
The most likely implementation would be some form of checkpointing
so that clients can explicitly ask for a checkpoint object and then
restore back to that checkpoint.
The checkpoint object could be complex if you have a series of
stacked read filters plus state in the format handler itself.

= MMap and async I/O performance experiments =

It should be possible to read archives using mmap()
or async I/O and it might be significantly faster than
the current approach.
However, this requires some careful performance testing
before we can be confident that it really is an improvement.
(If it doesn't help performance, we shouldn't do it.)

Async I/O in particular has promise for improving performance
with streaming tape drives.
Joerg Schilling has had good results with _star_ using two processes
and a shared-memory buffer to smooth out data flow when talking to tape drives.
I think async I/O could provide comparable performance
without forking. (Clients can easily get confused when
callbacks get invoked in different processes, so I'm reluctant
to fork within libarchive.  The first iteration of this
kind of buffering could certainly be done in bsdtar but
I'd like to eventually see it pushed down into the library.)

Similar improvements when writing archives are feasible,
though there are a few issues in the current write blocking that
make this harder than it sounds.
(The stackable write filter project above would
help; that would at least consolidate blocking into
fewer places.)

= ISO writer =

ISO writing is tricky because all metadata precedes all file data.
I've figured out an easy workaround, though, so I think this
is manageable:  Just write a single temporary file that
holds the bodies (each body padded out to a 2k sector
boundary), while accumulating metadata in
memory (including the offset in the temporary file of the
body data, of course).
At end-of-archive, construct the directory tree and
write that out, then copy the temp file after it.

The hardest part will be implementing this in a way that avoids the
need to have a lot of options.  The `mkisofs` and `mkhybrid` utilities
already do a good job of providing lots of options for people who need specific
control over how their ISO images are created; I'd like for libarchive
to provide a solid, easy-to-use answer for the 99% of the time
that people just need an ISO image that works.
(I envision that libarchive's ISO writer would always generate Joliet
and Rockridge extensions and would provide a very few
options for setting a label and making the image bootable.)

= Mac "!CopyFile()" support =

It would be nice for bsdtar to be compatible with Apple's
modified GNU tar implementation.

= xar reader and writer =

Xar should be relatively straightforward to read.
Writing will require tricks similar to the ISO writer.

= BSD-style long filename support for GNU ld =

The GNU/SysV ar format is ugly to write because you need to collect a filename table in advance.
This complicates programs that write ar format.
The BSD ar format avoids this problem but GNU ld doesn't support it.
If GNU ld could read the BSD ar format, then it would be easier
to create library-management tools on top of libarchive.